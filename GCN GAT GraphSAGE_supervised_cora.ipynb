{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch[13:14:09] /tmp/dgl_src/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: dlopen(/Users/fangzeyu/opt/anaconda3/envs/dgl/lib/python3.8/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.1.dylib, 0x0001): tried: '/Users/fangzeyu/opt/anaconda3/envs/dgl/lib/python3.8/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.1.dylib' (no such file)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ======== torch ========\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics.functional as MF\n",
    "\n",
    "# ======== DGL ========\n",
    "import dgl\n",
    "import dgl.data\n",
    "from dgl.nn import GraphConv, SAGEConv, GATConv\n",
    "\n",
    "import time\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n",
      "type of dataset: <class 'dgl.data.citation_graph.CoraGraphDataset'>\n",
      "type of g: <class 'dgl.heterograph.DGLHeteroGraph'>\n"
     ]
    }
   ],
   "source": [
    "# ======== 数据 ========\n",
    "# 用DGL自带的数据，直接导入DGLGraph类\n",
    "dataset = dgl.data.CoraGraphDataset() \n",
    "g = dataset[0] \n",
    "\n",
    "print('type of dataset:', type(dataset)) # <class 'dgl.data.citation_graph.CoraGraphDataset'>\n",
    "print('type of g:', type(g)) #  <class 'dgl.heterograph.DGLHeteroGraph'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=2708, num_edges=10556,\n",
      "      ndata_schemes={'feat': Scheme(shape=(1433,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'test_mask': Scheme(shape=(), dtype=torch.bool), 'train_mask': Scheme(shape=(), dtype=torch.bool)}\n",
      "      edata_schemes={})\n"
     ]
    }
   ],
   "source": [
    "# feat, label, val_mask, test_mask, train_mask 5个类别的节点属性\n",
    "print(g)\n",
    "# ndata, edata是graph属性的接口，Cora数据集中边没有属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== 采用DGL预定义的Module ========\n",
    "# 分别实现GCN、GAT、GraphSAGE模型，参考论文的实验设置\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, out_feats):\n",
    "        # 调用父类初始化函数\n",
    "        super(GCN, self).__init__() \n",
    "        \n",
    "        # 定义子模块\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = GraphConv(h_feats, out_feats)\n",
    "        \n",
    "    def forward(self, g, in_feat):\n",
    "        # 调用子模块\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        # h = F.log_softmax(h, 1)\n",
    "        return h\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        # 调用父类初始化函数\n",
    "        super(GAT, self).__init__() \n",
    "        \n",
    "        # 定义子模块\n",
    "        # The first layer consists of K = 8 attention heads computing F ′ = 8 features each (for a total of 64 features)\n",
    "        self.conv1 = GATConv(in_feats, 8, num_heads=8)\n",
    "        # The second layer is used for classification: a single attention head that computes C features\n",
    "        self.conv2 = GATConv(8*8, out_feats, num_heads=1)\n",
    "        \n",
    "    def forward(self, g, in_feat):\n",
    "        # 调用子模块\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = torch.flatten(h, 1)\n",
    "        h = F.elu(h) # followed by an exponential linear unit (ELU)\n",
    "        h = self.conv2(g, h)\n",
    "        h = torch.mean(h, 1)\n",
    "        # h = F.log_softmax(h, 1)\n",
    "        return h\n",
    "    \n",
    "    \n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, out_feats, aggregator_type):\n",
    "        # 调用父类初始化函数\n",
    "        super(GraphSAGE, self).__init__() \n",
    "        \n",
    "        # 定义子模块\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, aggregator_type)\n",
    "        self.conv2 = SAGEConv(h_feats, out_feats, aggregator_type)\n",
    "        \n",
    "    def forward(self, g, in_feat):\n",
    "        # 调用子模块\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g, model, epochs, lr, weight_decay=0):\n",
    "    # optimizer, loss_fn = F.cross_entropy\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # graph data    \n",
    "    features = g.ndata['feat']\n",
    "    labels = g.ndata['label']\n",
    "    train_mask = g.ndata['train_mask']\n",
    "    val_mask = g.ndata['val_mask']\n",
    "    test_mask = g.ndata['test_mask']\n",
    "    \n",
    "    # best metric score\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "    \n",
    "    dur = []\n",
    "    \n",
    "    # train loop\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        if e >= 3:\n",
    "            t0 = time.time()\n",
    "        \n",
    "        # Forward\n",
    "        logits = model(g, features)\n",
    "        pred = logits.argmax(axis=1)\n",
    "        \n",
    "        # loss 这里input参数用的是logits，而不是pred，具体查询cross_entropy的参数规定\n",
    "        loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Compute accuracy\n",
    "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
    "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
    "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
    "        \n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            \n",
    "        if e >= 3: \n",
    "            dur.append(time.time() - t0)\n",
    "            \n",
    "        if (e + 1) % 10 == 0:\n",
    "            print('Epoch {:2d} | Loss {:.3f} | Train Acc {:.3f} | '.format(e, loss, train_acc),\n",
    "                  'Val Acc {:.3f} (best: {:.3f}) | '.format(val_acc, best_val_acc),\n",
    "                  'Test Acc {:.3f} (best: {:.3f}) | Time (s) {:.4f}'.format(test_acc, best_test_acc, np.mean(dur))\n",
    "                  )\n",
    "            \n",
    "    print('Total Time (s): {:.4f}, Best Val Acc: {:.4f}, Best Test Acc: {:.4f}'.format(dur[-1], best_val_acc, best_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fangzeyu/opt/anaconda3/envs/dgl/lib/python3.8/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9 | Loss 1.850 | Train Acc 0.936 |  Val Acc 0.628 (best: 0.668) |  Test Acc 0.629 (best: 0.649) | Time (s) 0.0260\n",
      "Epoch 19 | Loss 1.663 | Train Acc 0.950 |  Val Acc 0.688 (best: 0.688) |  Test Acc 0.674 (best: 0.674) | Time (s) 0.0260\n",
      "Epoch 29 | Loss 1.412 | Train Acc 0.957 |  Val Acc 0.746 (best: 0.746) |  Test Acc 0.756 (best: 0.756) | Time (s) 0.0265\n",
      "Epoch 39 | Loss 1.135 | Train Acc 0.971 |  Val Acc 0.764 (best: 0.764) |  Test Acc 0.779 (best: 0.779) | Time (s) 0.0264\n",
      "Epoch 49 | Loss 0.885 | Train Acc 0.971 |  Val Acc 0.782 (best: 0.784) |  Test Acc 0.800 (best: 0.796) | Time (s) 0.0269\n",
      "Epoch 59 | Loss 0.695 | Train Acc 0.971 |  Val Acc 0.784 (best: 0.786) |  Test Acc 0.803 (best: 0.805) | Time (s) 0.0271\n",
      "Epoch 69 | Loss 0.565 | Train Acc 0.986 |  Val Acc 0.780 (best: 0.786) |  Test Acc 0.811 (best: 0.805) | Time (s) 0.0270\n",
      "Epoch 79 | Loss 0.478 | Train Acc 0.993 |  Val Acc 0.772 (best: 0.786) |  Test Acc 0.811 (best: 0.805) | Time (s) 0.0269\n",
      "Epoch 89 | Loss 0.417 | Train Acc 0.993 |  Val Acc 0.770 (best: 0.786) |  Test Acc 0.811 (best: 0.805) | Time (s) 0.0270\n",
      "Epoch 99 | Loss 0.372 | Train Acc 0.993 |  Val Acc 0.776 (best: 0.786) |  Test Acc 0.808 (best: 0.805) | Time (s) 0.0269\n",
      "Total Time (s): 0.0266, Best Val Acc: 0.7860, Best Test Acc: 0.8050\n"
     ]
    }
   ],
   "source": [
    "# Ceate the model\n",
    "model = GCN(g.ndata['feat'].shape[1], 64, dataset.num_classes)\n",
    "train(g, model, epochs=100, lr=5e-3, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9 | Loss 1.745 | Train Acc 0.979 |  Val Acc 0.778 (best: 0.780) |  Test Acc 0.780 (best: 0.779) | Time (s) 0.0363\n",
      "Epoch 19 | Loss 1.470 | Train Acc 0.964 |  Val Acc 0.778 (best: 0.780) |  Test Acc 0.775 (best: 0.779) | Time (s) 0.0390\n",
      "Epoch 29 | Loss 1.161 | Train Acc 0.964 |  Val Acc 0.782 (best: 0.782) |  Test Acc 0.788 (best: 0.783) | Time (s) 0.0405\n",
      "Epoch 39 | Loss 0.868 | Train Acc 0.971 |  Val Acc 0.778 (best: 0.784) |  Test Acc 0.791 (best: 0.788) | Time (s) 0.0402\n",
      "Epoch 49 | Loss 0.639 | Train Acc 0.986 |  Val Acc 0.774 (best: 0.784) |  Test Acc 0.791 (best: 0.788) | Time (s) 0.0393\n",
      "Epoch 59 | Loss 0.486 | Train Acc 0.993 |  Val Acc 0.774 (best: 0.784) |  Test Acc 0.798 (best: 0.788) | Time (s) 0.0379\n",
      "Epoch 69 | Loss 0.390 | Train Acc 0.993 |  Val Acc 0.790 (best: 0.790) |  Test Acc 0.800 (best: 0.800) | Time (s) 0.0370\n",
      "Epoch 79 | Loss 0.323 | Train Acc 0.993 |  Val Acc 0.780 (best: 0.790) |  Test Acc 0.792 (best: 0.800) | Time (s) 0.0364\n",
      "Epoch 89 | Loss 0.273 | Train Acc 1.000 |  Val Acc 0.778 (best: 0.790) |  Test Acc 0.788 (best: 0.800) | Time (s) 0.0358\n",
      "Epoch 99 | Loss 0.232 | Train Acc 1.000 |  Val Acc 0.766 (best: 0.790) |  Test Acc 0.784 (best: 0.800) | Time (s) 0.0357\n",
      "Total Time (s): 0.0348, Best Val Acc: 0.7900, Best Test Acc: 0.8000\n"
     ]
    }
   ],
   "source": [
    "# Ceate the model\n",
    "model = GAT(g.ndata['feat'].shape[1], dataset.num_classes)\n",
    "train(g, model, epochs=100, lr=5e-3, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9 | Loss 1.798 | Train Acc 0.950 |  Val Acc 0.738 (best: 0.738) |  Test Acc 0.736 (best: 0.736) | Time (s) 0.0290\n",
      "Epoch 19 | Loss 1.551 | Train Acc 0.957 |  Val Acc 0.778 (best: 0.780) |  Test Acc 0.778 (best: 0.778) | Time (s) 0.0259\n",
      "Epoch 29 | Loss 1.260 | Train Acc 0.957 |  Val Acc 0.784 (best: 0.786) |  Test Acc 0.794 (best: 0.786) | Time (s) 0.0247\n",
      "Epoch 39 | Loss 0.975 | Train Acc 0.971 |  Val Acc 0.794 (best: 0.794) |  Test Acc 0.802 (best: 0.802) | Time (s) 0.0248\n",
      "Epoch 49 | Loss 0.742 | Train Acc 0.979 |  Val Acc 0.798 (best: 0.798) |  Test Acc 0.807 (best: 0.808) | Time (s) 0.0246\n",
      "Epoch 59 | Loss 0.577 | Train Acc 0.986 |  Val Acc 0.798 (best: 0.798) |  Test Acc 0.814 (best: 0.808) | Time (s) 0.0246\n",
      "Epoch 69 | Loss 0.469 | Train Acc 0.986 |  Val Acc 0.798 (best: 0.802) |  Test Acc 0.822 (best: 0.817) | Time (s) 0.0244\n",
      "Epoch 79 | Loss 0.399 | Train Acc 0.986 |  Val Acc 0.800 (best: 0.802) |  Test Acc 0.819 (best: 0.817) | Time (s) 0.0244\n",
      "Epoch 89 | Loss 0.350 | Train Acc 0.993 |  Val Acc 0.794 (best: 0.802) |  Test Acc 0.817 (best: 0.817) | Time (s) 0.0241\n",
      "Epoch 99 | Loss 0.314 | Train Acc 0.993 |  Val Acc 0.794 (best: 0.802) |  Test Acc 0.821 (best: 0.817) | Time (s) 0.0240\n",
      "Total Time (s): 0.0249, Best Val Acc: 0.8020, Best Test Acc: 0.8170\n"
     ]
    }
   ],
   "source": [
    "# Ceate the model\n",
    "model = GraphSAGE(g.ndata['feat'].shape[1], 64, dataset.num_classes, aggregator_type='gcn')\n",
    "train(g, model, epochs=100, lr=5e-3, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= 实现GCN =======\n",
    "# Refer: https://zhuanlan.zhihu.com/p/139359188\n",
    "# 定义消息函数(message)、聚合函数(reduce)和更新函数(apply)的用户自定义函数(UDF)\n",
    "\n",
    "def gcn_message(edges):\n",
    "    \"\"\"\n",
    "    消息函数(message)：从源节点向目标节点传递信息，存储在目标节点的信箱(mailbox)中。\n",
    "\n",
    "    Args:\n",
    "        edges (EdgeBatch): 表示一批边，包括src、dst、data三个成员属性。\n",
    "\n",
    "    Return:\n",
    "        _ (Dictionary): 传递出的消息，键值表示字段名。\n",
    "    \"\"\"\n",
    "    \n",
    "    msg = edges.src['h'] * edges.src['norm'] # 按位置相乘\n",
    "    return {'m': msg}\n",
    "    \n",
    "def gcn_reduce(nodes):\n",
    "    \"\"\"\n",
    "    聚合函数(reduce)：处理节点信箱(mailbox)中收到的消息。\n",
    "\n",
    "    Args:\n",
    "        nodes (NodeBatch): 表示一批节点，包括data、mailbox两个成员属性。\n",
    "        \n",
    "    Return:\n",
    "        _ (Dictionary): 处理后的消息，键值表示字段名。\n",
    "    \"\"\"\n",
    "    \n",
    "    sum = torch.sum(nodes.mailbox['m'], dim=1) # dim=0是不同的节点，dim=1是每个节点收到的消息\n",
    "    h = sum * nodes.data['norm']\n",
    "    return {'h': h}\n",
    "    \n",
    "    \n",
    "# 定义 GCN Layer    \n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, dropout, bias=True):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.Tensor(in_feats, out_feats))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_feats))\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.reset_parameter()\n",
    "        \n",
    "        if dropout is not None:\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "        \n",
    "    def reset_parameter(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))   \n",
    "        self.weight.data.uniform_(-stdv, stdv) \n",
    "        if self.bias is not None:\n",
    "            stdv = 1. / math.sqrt(self.bias.size(0))\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "    def forward(self, g, in_feat):\n",
    "        with g.local_scope():\n",
    "            if self.dropout is not None:\n",
    "                h = self.dropout(in_feat)\n",
    "            else:\n",
    "                h = in_feat\n",
    "            g.ndata['h'] = torch.mm(h, self.weight)\n",
    "            g.update_all(gcn_message, gcn_reduce)\n",
    "            h = g.ndata['h']\n",
    "            if self.bias is not None:\n",
    "                h = h + self.bias\n",
    "            return h\n",
    "        \n",
    "        \n",
    "class GCN_by_hand(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, out_feats, dropout=None, bias=True):\n",
    "        super(GCN_by_hand, self).__init__()\n",
    "        \n",
    "        # 定义子模块\n",
    "        self.conv1 = GCNLayer(in_feats, h_feats, dropout=dropout, bias=bias)\n",
    "        self.conv2 = GCNLayer(h_feats, out_feats, dropout=dropout, bias=bias)\n",
    "        \n",
    "    def forward(self, g, in_feat):\n",
    "        # 调用子模块\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9 | Loss 1.854 | Train Acc 0.421 |  Val Acc 0.254 (best: 0.254) |  Test Acc 0.239 (best: 0.239) | Time (s) 0.0774\n",
      "Epoch 19 | Loss 1.714 | Train Acc 0.743 |  Val Acc 0.498 (best: 0.498) |  Test Acc 0.514 (best: 0.514) | Time (s) 0.0754\n",
      "Epoch 29 | Loss 1.519 | Train Acc 0.900 |  Val Acc 0.682 (best: 0.682) |  Test Acc 0.699 (best: 0.699) | Time (s) 0.0732\n",
      "Epoch 39 | Loss 1.281 | Train Acc 0.957 |  Val Acc 0.744 (best: 0.748) |  Test Acc 0.771 (best: 0.767) | Time (s) 0.0755\n",
      "Epoch 49 | Loss 1.033 | Train Acc 0.971 |  Val Acc 0.770 (best: 0.770) |  Test Acc 0.799 (best: 0.799) | Time (s) 0.0753\n",
      "Epoch 59 | Loss 0.814 | Train Acc 0.979 |  Val Acc 0.778 (best: 0.778) |  Test Acc 0.810 (best: 0.808) | Time (s) 0.0748\n",
      "Epoch 69 | Loss 0.646 | Train Acc 0.986 |  Val Acc 0.776 (best: 0.778) |  Test Acc 0.812 (best: 0.808) | Time (s) 0.0785\n",
      "Epoch 79 | Loss 0.530 | Train Acc 0.993 |  Val Acc 0.772 (best: 0.782) |  Test Acc 0.815 (best: 0.813) | Time (s) 0.0807\n",
      "Epoch 89 | Loss 0.450 | Train Acc 0.993 |  Val Acc 0.774 (best: 0.782) |  Test Acc 0.816 (best: 0.813) | Time (s) 0.0787\n",
      "Epoch 99 | Loss 0.394 | Train Acc 0.993 |  Val Acc 0.774 (best: 0.782) |  Test Acc 0.814 (best: 0.813) | Time (s) 0.0775\n",
      "Total Time (s): 0.0669, Best Val Acc: 0.7820, Best Test Acc: 0.8130\n"
     ]
    }
   ],
   "source": [
    "with g.local_scope():\n",
    "    # 归一化入度\n",
    "    degs = g.in_degrees().float()\n",
    "    norm = torch.pow(degs, -0.5)\n",
    "    norm[torch.isinf(norm)] = 0\n",
    "    g.ndata['norm'] = norm.unsqueeze(1)\n",
    "\n",
    "    # Ceate the model\n",
    "    model = GCN_by_hand(g.ndata['feat'].shape[1], 64, dataset.num_classes, bias=True)\n",
    "    train(g, model, epochs=100, lr=5e-3, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 实现GAT ======\n",
    "# Refer: https://docs.dgl.ai/tutorials/models/1_gnn/9_gat.html\n",
    "\n",
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super(GATLayer, self).__init__()\n",
    "        \n",
    "        # 定义子模块\n",
    "        self.fc = nn.Linear(in_feats, out_feats, bias=False)\n",
    "        self.atten_fc = nn.Linear(2 * out_feats, 1, bias=False)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        nn.init.xavier_normal_(self.fc.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.atten_fc.weight, gain=gain)\n",
    "        \n",
    "    def gat_edge_apply(self, edges):\n",
    "        z2 = torch.concat([edges.src['z'], edges.dst['z']], dim=1)\n",
    "        a = self.atten_fc(z2)\n",
    "        return {'e': F.leaky_relu(a)}\n",
    "    \n",
    "    def gat_message(self, edges):\n",
    "        return {'z': edges.src['z'], 'e': edges.data['e']}\n",
    "    \n",
    "    def gat_reduce(self, nodes):\n",
    "        alpha = F.softmax(nodes.mailbox['e'], dim=1)\n",
    "        h = torch.sum(alpha * nodes.mailbox['z'], dim=1)\n",
    "        return {'h': h}\n",
    "        \n",
    "    def forward(self, g, in_feat):\n",
    "        with g.local_scope():\n",
    "            z = self.fc(in_feat)\n",
    "            g.ndata['z'] = z\n",
    "            g.apply_edges(self.gat_edge_apply)\n",
    "            g.update_all(self.gat_message, self.gat_reduce)\n",
    "            return g.ndata['h']\n",
    "            \n",
    "            \n",
    "class MultiHeadGATLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, num_heads, merge='cat'):\n",
    "        super(MultiHeadGATLayer, self).__init__()\n",
    "        self.heads = nn.ModuleList()\n",
    "        for i in range(num_heads):\n",
    "            self.heads.append(GATLayer(in_feats, out_feats))\n",
    "        self.merge = merge\n",
    "        \n",
    "    def forward(self, g, in_feat):\n",
    "        head_outs = [head(g, in_feat) for head in self.heads]\n",
    "        \n",
    "        if self.merge == 'cat':\n",
    "            return torch.cat(head_outs, dim=1)\n",
    "        else:\n",
    "            return torch.mean(torch.stack(head_outs), dim=0)\n",
    "        \n",
    "        \n",
    "class GAT_by_hand(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super(GAT_by_hand, self).__init__()\n",
    "        \n",
    "        # 定义子模块\n",
    "        self.conv1 = MultiHeadGATLayer(in_feats, 8, num_heads=8)\n",
    "        self.conv2 = MultiHeadGATLayer(8*8, out_feats, num_heads=1, merge='avg')\n",
    "        \n",
    "    def forward(self, g, in_feat):\n",
    "        # 调用子模块\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.elu(h) \n",
    "        h = self.conv2(g, h)\n",
    "        # h = torch.mean(h, 1)\n",
    "        # h = F.log_softmax(h, 1)\n",
    "        return h\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9 | Loss 1.738 | Train Acc 0.957 |  Val Acc 0.750 (best: 0.750) |  Test Acc 0.746 (best: 0.746) | Time (s) 0.3104\n",
      "Epoch 19 | Loss 1.465 | Train Acc 0.964 |  Val Acc 0.766 (best: 0.766) |  Test Acc 0.778 (best: 0.772) | Time (s) 0.3040\n",
      "Epoch 29 | Loss 1.167 | Train Acc 0.964 |  Val Acc 0.776 (best: 0.776) |  Test Acc 0.784 (best: 0.783) | Time (s) 0.2976\n",
      "Epoch 39 | Loss 0.887 | Train Acc 0.971 |  Val Acc 0.782 (best: 0.782) |  Test Acc 0.790 (best: 0.789) | Time (s) 0.2998\n",
      "Epoch 49 | Loss 0.660 | Train Acc 0.979 |  Val Acc 0.784 (best: 0.784) |  Test Acc 0.795 (best: 0.792) | Time (s) 0.3066\n",
      "Epoch 59 | Loss 0.498 | Train Acc 0.993 |  Val Acc 0.782 (best: 0.786) |  Test Acc 0.787 (best: 0.795) | Time (s) 0.3028\n",
      "Epoch 69 | Loss 0.391 | Train Acc 0.993 |  Val Acc 0.772 (best: 0.786) |  Test Acc 0.781 (best: 0.795) | Time (s) 0.2998\n",
      "Epoch 79 | Loss 0.326 | Train Acc 0.993 |  Val Acc 0.768 (best: 0.786) |  Test Acc 0.770 (best: 0.795) | Time (s) 0.2980\n",
      "Epoch 89 | Loss 0.283 | Train Acc 1.000 |  Val Acc 0.764 (best: 0.786) |  Test Acc 0.768 (best: 0.795) | Time (s) 0.2967\n",
      "Epoch 99 | Loss 0.253 | Train Acc 1.000 |  Val Acc 0.762 (best: 0.786) |  Test Acc 0.761 (best: 0.795) | Time (s) 0.2953\n",
      "Total Time (s): 0.2827, Best Val Acc: 0.7860, Best Test Acc: 0.7950\n"
     ]
    }
   ],
   "source": [
    "# Ceate the model\n",
    "model = GAT_by_hand(g.ndata['feat'].shape[1], dataset.num_classes)\n",
    "train(g, model, epochs=100, lr=5e-3, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== GraphSAGE实现 ======\n",
    "# Refer: \n",
    "#   1. https://github.com/dmlc/dgl/blob/master/examples/pytorch/graphsage/node_classification.py \n",
    "#   2. https://docs.dgl.ai/guide_cn/minibatch-node.html#guide-cn-minibatch-node-classification-sampler\n",
    "\n",
    "class GraphSAGE_batch(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, out_feats):\n",
    "        super(GraphSAGE_batch, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, 'gcn')\n",
    "        self.conv2 = SAGEConv(h_feats, out_feats, 'gcn')\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.h_feats = h_feats\n",
    "        self.out_feats = out_feats\n",
    "        self.n_layers = 2\n",
    "\n",
    "    def forward(self, blocks, in_feat):\n",
    "        h = in_feat\n",
    "        h = self.conv1(blocks[0], h)\n",
    "        h = F.relu(h)\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv2(blocks[1], h)\n",
    "        return h\n",
    "    \n",
    "    def inference(self, g, device, batch_size):\n",
    "        # 用该模块进行离线推断，在GPU显存有限的情况下通过小批次处理和邻居采样实现全图前向传播的方法\n",
    "        # Refer: https://docs.dgl.ai/guide_cn/minibatch-inference.html\n",
    "        g.ndata['h'] = g.ndata['feat']\n",
    "        \n",
    "        sampler = dgl.dataloading.MultiLayerFullNeighborSampler(1)\n",
    "        dataloader = dgl.dataloading.NodeDataLoader(\n",
    "            g, torch.arange(g.num_nodes()).to(g.device), sampler, device=device,\n",
    "            batch_size=batch_size, shuffle=False, drop_last=False\n",
    "        )\n",
    "        \n",
    "        # 推断算法将包含一个外循环以迭代执行各层，和一个内循环以迭代处理各个节点小批次。\n",
    "        for l, layer in enumerate([self.conv1, self.conv2]):\n",
    "            y = torch.zeros(g.num_nodes(), self.h_feats if l != self.n_layers - 1 else self.out_feats)\n",
    "            \n",
    "            for input_nodes, output_nodes, blocks in dataloader:\n",
    "                # 计算输出\n",
    "                block = blocks[0]\n",
    "                x = block.srcdata['h']\n",
    "                h = layer(block, x)\n",
    "                if l != self.n_layers - 1:\n",
    "                    h = F.relu(h)\n",
    "                    h = self.dropout(h)\n",
    "\n",
    "                y[output_nodes] = h\n",
    "                \n",
    "            g.ndata['h'] = y\n",
    "            \n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(g, model, epochs, lr, batch_size, weight_decay=0, device='cpu'):\n",
    "    # optimizer, loss_fn = F.cross_entropy\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # graph data    \n",
    "    features = g.ndata['feat']\n",
    "    labels = g.ndata['label']\n",
    "    train_mask = g.ndata['train_mask']\n",
    "    val_mask = g.ndata['val_mask']\n",
    "    test_mask = g.ndata['test_mask']\n",
    "    \n",
    "    train_nids = g.nodes()[train_mask]\n",
    "    val_nids = g.nodes()[val_mask]\n",
    "    test_nids = g.nodes()[test_mask]\n",
    "    \n",
    "    # 定义邻居采样器和dataloader\n",
    "    sampler = dgl.dataloading.MultiLayerNeighborSampler([25, 10])\n",
    "    train_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "        g, train_nids, sampler, device=device, batch_size=batch_size, \n",
    "        shuffle=False, drop_last=False\n",
    "    )\n",
    "    val_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "        g, val_nids, sampler, device=device, batch_size=batch_size, \n",
    "        shuffle=False, drop_last=False\n",
    "    )\n",
    "    \n",
    "    # best metric score\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "    \n",
    "    dur = []\n",
    "    \n",
    "    # train loop\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        if e >= 3:\n",
    "            t0 = time.time()\n",
    "        \n",
    "        for it, (input_nodes, output_nodes, blocks) in enumerate(train_dataloader):\n",
    "            x = blocks[0].srcdata['feat']\n",
    "            y = blocks[-1].dstdata['label']\n",
    "            \n",
    "            # Forward\n",
    "            y_hat = model(blocks, x)\n",
    "            \n",
    "            # Loss\n",
    "            loss = F.cross_entropy(y_hat, y)\n",
    "        \n",
    "            # Backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Output\n",
    "            if it % 10 == 0:\n",
    "                train_acc = MF.accuracy(y_hat, y) # Compute accuracy\n",
    "                print('Loss {:.4f} | Train Acc {:.4f}'.format(loss, train_acc))\n",
    "        \n",
    "        if e >= 3: \n",
    "            dur.append(time.time() - t0)\n",
    "        \n",
    "        model.eval()\n",
    "        ys = []\n",
    "        y_hats = []\n",
    "        for it, (input_nodes, output_nodes, blocks) in enumerate(val_dataloader):\n",
    "            with torch.no_grad():\n",
    "                x = blocks[0].srcdata['feat']\n",
    "                ys.append(blocks[-1].dstdata['label'])\n",
    "                y_hats.append(model(blocks, x))\n",
    "        \n",
    "        val_acc = MF.accuracy(torch.cat(y_hats), torch.cat(ys))\n",
    "        \n",
    "        print('Epoch {:3d} | Val acc: {:.4f} '.format(e, val_acc.item()), '| Times (s): {:.4f}'.format(np.mean(dur)))\n",
    "        \n",
    "    # Test accuracy and offline inference of all nodes\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model.inference(g, device=device, batch_size=batch_size)\n",
    "        acc = MF.accuracy(pred[test_mask], labels[test_mask])\n",
    "        print('Test acc {:.4f}'.format(acc.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 1.9460 | Train Acc 0.1429\n",
      "Epoch   0 | Val acc: 0.4160  | Times (s): nan\n",
      "Loss 1.9235 | Train Acc 0.6714\n",
      "Epoch   1 | Val acc: 0.6300  | Times (s): nan\n",
      "Loss 1.9022 | Train Acc 0.8571\n",
      "Epoch   2 | Val acc: 0.7480  | Times (s): nan\n",
      "Loss 1.8823 | Train Acc 0.8786\n",
      "Epoch   3 | Val acc: 0.7540  | Times (s): 0.0496\n",
      "Loss 1.8546 | Train Acc 0.9429\n",
      "Epoch   4 | Val acc: 0.7640  | Times (s): 0.0478\n",
      "Loss 1.8304 | Train Acc 0.9571\n",
      "Epoch   5 | Val acc: 0.7780  | Times (s): 0.0474\n",
      "Loss 1.7946 | Train Acc 0.9714\n",
      "Epoch   6 | Val acc: 0.7780  | Times (s): 0.0461\n",
      "Loss 1.7661 | Train Acc 0.9714\n",
      "Epoch   7 | Val acc: 0.7860  | Times (s): 0.0451\n",
      "Loss 1.7247 | Train Acc 0.9429\n",
      "Epoch   8 | Val acc: 0.7840  | Times (s): 0.0447\n",
      "Loss 1.6930 | Train Acc 0.9357\n",
      "Epoch   9 | Val acc: 0.7840  | Times (s): 0.0447\n",
      "Loss 1.6564 | Train Acc 0.9643\n",
      "Epoch  10 | Val acc: 0.7900  | Times (s): 0.0446\n",
      "Loss 1.6121 | Train Acc 0.9714\n",
      "Epoch  11 | Val acc: 0.7880  | Times (s): 0.0447\n",
      "Loss 1.5651 | Train Acc 0.9714\n",
      "Epoch  12 | Val acc: 0.7860  | Times (s): 0.0448\n",
      "Loss 1.5301 | Train Acc 0.9643\n",
      "Epoch  13 | Val acc: 0.7880  | Times (s): 0.0447\n",
      "Loss 1.4837 | Train Acc 0.9786\n",
      "Epoch  14 | Val acc: 0.7860  | Times (s): 0.0446\n",
      "Loss 1.4441 | Train Acc 0.9643\n",
      "Epoch  15 | Val acc: 0.7900  | Times (s): 0.0446\n",
      "Loss 1.3885 | Train Acc 0.9429\n",
      "Epoch  16 | Val acc: 0.7840  | Times (s): 0.0446\n",
      "Loss 1.3465 | Train Acc 0.9571\n",
      "Epoch  17 | Val acc: 0.7840  | Times (s): 0.0448\n",
      "Loss 1.2939 | Train Acc 0.9500\n",
      "Epoch  18 | Val acc: 0.7840  | Times (s): 0.0445\n",
      "Loss 1.2457 | Train Acc 0.9571\n",
      "Epoch  19 | Val acc: 0.7800  | Times (s): 0.0442\n",
      "Loss 1.1996 | Train Acc 0.9643\n",
      "Epoch  20 | Val acc: 0.7840  | Times (s): 0.0441\n",
      "Loss 1.1457 | Train Acc 0.9429\n",
      "Epoch  21 | Val acc: 0.7880  | Times (s): 0.0441\n",
      "Loss 1.0950 | Train Acc 0.9500\n",
      "Epoch  22 | Val acc: 0.7900  | Times (s): 0.0440\n",
      "Loss 1.0457 | Train Acc 0.9714\n",
      "Epoch  23 | Val acc: 0.7880  | Times (s): 0.0455\n",
      "Loss 1.0022 | Train Acc 0.9643\n",
      "Epoch  24 | Val acc: 0.7880  | Times (s): 0.0456\n",
      "Loss 0.9567 | Train Acc 0.9714\n",
      "Epoch  25 | Val acc: 0.7880  | Times (s): 0.0457\n",
      "Loss 0.8983 | Train Acc 0.9786\n",
      "Epoch  26 | Val acc: 0.7900  | Times (s): 0.0457\n",
      "Loss 0.8706 | Train Acc 0.9643\n",
      "Epoch  27 | Val acc: 0.7920  | Times (s): 0.0456\n",
      "Loss 0.8249 | Train Acc 0.9857\n",
      "Epoch  28 | Val acc: 0.7940  | Times (s): 0.0454\n",
      "Loss 0.7857 | Train Acc 0.9643\n",
      "Epoch  29 | Val acc: 0.7980  | Times (s): 0.0452\n",
      "Loss 0.7479 | Train Acc 0.9571\n",
      "Epoch  30 | Val acc: 0.7960  | Times (s): 0.0469\n",
      "Loss 0.7145 | Train Acc 0.9786\n",
      "Epoch  31 | Val acc: 0.8020  | Times (s): 0.0471\n",
      "Loss 0.6873 | Train Acc 0.9857\n",
      "Epoch  32 | Val acc: 0.8020  | Times (s): 0.0470\n",
      "Loss 0.6562 | Train Acc 0.9714\n",
      "Epoch  33 | Val acc: 0.8000  | Times (s): 0.0468\n",
      "Loss 0.6204 | Train Acc 0.9571\n",
      "Epoch  34 | Val acc: 0.7980  | Times (s): 0.0467\n",
      "Loss 0.5952 | Train Acc 0.9857\n",
      "Epoch  35 | Val acc: 0.8020  | Times (s): 0.0466\n",
      "Loss 0.5775 | Train Acc 0.9786\n",
      "Epoch  36 | Val acc: 0.8040  | Times (s): 0.0466\n",
      "Loss 0.5471 | Train Acc 0.9929\n",
      "Epoch  37 | Val acc: 0.8040  | Times (s): 0.0466\n",
      "Loss 0.4934 | Train Acc 0.9786\n",
      "Epoch  38 | Val acc: 0.8040  | Times (s): 0.0466\n",
      "Loss 0.5025 | Train Acc 0.9857\n",
      "Epoch  39 | Val acc: 0.8040  | Times (s): 0.0466\n",
      "Loss 0.4713 | Train Acc 0.9857\n",
      "Epoch  40 | Val acc: 0.8080  | Times (s): 0.0464\n",
      "Loss 0.4558 | Train Acc 0.9643\n",
      "Epoch  41 | Val acc: 0.8080  | Times (s): 0.0464\n",
      "Loss 0.4322 | Train Acc 0.9857\n",
      "Epoch  42 | Val acc: 0.8100  | Times (s): 0.0464\n",
      "Loss 0.4241 | Train Acc 0.9857\n",
      "Epoch  43 | Val acc: 0.8080  | Times (s): 0.0463\n",
      "Loss 0.4062 | Train Acc 0.9929\n",
      "Epoch  44 | Val acc: 0.8060  | Times (s): 0.0462\n",
      "Loss 0.4041 | Train Acc 0.9857\n",
      "Epoch  45 | Val acc: 0.8060  | Times (s): 0.0461\n",
      "Loss 0.3896 | Train Acc 0.9857\n",
      "Epoch  46 | Val acc: 0.8020  | Times (s): 0.0459\n",
      "Loss 0.3939 | Train Acc 0.9857\n",
      "Epoch  47 | Val acc: 0.8000  | Times (s): 0.0459\n",
      "Loss 0.3670 | Train Acc 0.9929\n",
      "Epoch  48 | Val acc: 0.8000  | Times (s): 0.0459\n",
      "Loss 0.3595 | Train Acc 0.9857\n",
      "Epoch  49 | Val acc: 0.8040  | Times (s): 0.0458\n",
      "Loss 0.3576 | Train Acc 0.9857\n",
      "Epoch  50 | Val acc: 0.8040  | Times (s): 0.0458\n",
      "Loss 0.3371 | Train Acc 0.9857\n",
      "Epoch  51 | Val acc: 0.7980  | Times (s): 0.0457\n",
      "Loss 0.3451 | Train Acc 0.9857\n",
      "Epoch  52 | Val acc: 0.7960  | Times (s): 0.0457\n",
      "Loss 0.3378 | Train Acc 0.9857\n",
      "Epoch  53 | Val acc: 0.7980  | Times (s): 0.0457\n",
      "Loss 0.3230 | Train Acc 0.9857\n",
      "Epoch  54 | Val acc: 0.8020  | Times (s): 0.0457\n",
      "Loss 0.3203 | Train Acc 0.9857\n",
      "Epoch  55 | Val acc: 0.8000  | Times (s): 0.0456\n",
      "Loss 0.3112 | Train Acc 0.9857\n",
      "Epoch  56 | Val acc: 0.8020  | Times (s): 0.0456\n",
      "Loss 0.3101 | Train Acc 0.9857\n",
      "Epoch  57 | Val acc: 0.8000  | Times (s): 0.0456\n",
      "Loss 0.3056 | Train Acc 0.9929\n",
      "Epoch  58 | Val acc: 0.7980  | Times (s): 0.0455\n",
      "Loss 0.2928 | Train Acc 0.9857\n",
      "Epoch  59 | Val acc: 0.8020  | Times (s): 0.0454\n",
      "Loss 0.3036 | Train Acc 0.9857\n",
      "Epoch  60 | Val acc: 0.7980  | Times (s): 0.0453\n",
      "Loss 0.2985 | Train Acc 0.9929\n",
      "Epoch  61 | Val acc: 0.7980  | Times (s): 0.0452\n",
      "Loss 0.2767 | Train Acc 0.9929\n",
      "Epoch  62 | Val acc: 0.7940  | Times (s): 0.0451\n",
      "Loss 0.2855 | Train Acc 0.9929\n",
      "Epoch  63 | Val acc: 0.8000  | Times (s): 0.0450\n",
      "Loss 0.2768 | Train Acc 0.9857\n",
      "Epoch  64 | Val acc: 0.7960  | Times (s): 0.0449\n",
      "Loss 0.2761 | Train Acc 0.9929\n",
      "Epoch  65 | Val acc: 0.7940  | Times (s): 0.0449\n",
      "Loss 0.2770 | Train Acc 0.9929\n",
      "Epoch  66 | Val acc: 0.7980  | Times (s): 0.0448\n",
      "Loss 0.2729 | Train Acc 0.9929\n",
      "Epoch  67 | Val acc: 0.7960  | Times (s): 0.0447\n",
      "Loss 0.2645 | Train Acc 1.0000\n",
      "Epoch  68 | Val acc: 0.7980  | Times (s): 0.0447\n",
      "Loss 0.2704 | Train Acc 0.9929\n",
      "Epoch  69 | Val acc: 0.7960  | Times (s): 0.0446\n",
      "Loss 0.2651 | Train Acc 0.9857\n",
      "Epoch  70 | Val acc: 0.7980  | Times (s): 0.0446\n",
      "Loss 0.2684 | Train Acc 0.9929\n",
      "Epoch  71 | Val acc: 0.7960  | Times (s): 0.0445\n",
      "Loss 0.2415 | Train Acc 0.9929\n",
      "Epoch  72 | Val acc: 0.7980  | Times (s): 0.0445\n",
      "Loss 0.2468 | Train Acc 0.9929\n",
      "Epoch  73 | Val acc: 0.7960  | Times (s): 0.0444\n",
      "Loss 0.2464 | Train Acc 0.9857\n",
      "Epoch  74 | Val acc: 0.7980  | Times (s): 0.0444\n",
      "Loss 0.2359 | Train Acc 0.9929\n",
      "Epoch  75 | Val acc: 0.7940  | Times (s): 0.0444\n",
      "Loss 0.2352 | Train Acc 0.9857\n",
      "Epoch  76 | Val acc: 0.7960  | Times (s): 0.0443\n",
      "Loss 0.2332 | Train Acc 1.0000\n",
      "Epoch  77 | Val acc: 0.7960  | Times (s): 0.0443\n",
      "Loss 0.2295 | Train Acc 0.9929\n",
      "Epoch  78 | Val acc: 0.7960  | Times (s): 0.0442\n",
      "Loss 0.2243 | Train Acc 1.0000\n",
      "Epoch  79 | Val acc: 0.7980  | Times (s): 0.0442\n",
      "Loss 0.2279 | Train Acc 0.9857\n",
      "Epoch  80 | Val acc: 0.7980  | Times (s): 0.0441\n",
      "Loss 0.2210 | Train Acc 0.9929\n",
      "Epoch  81 | Val acc: 0.7940  | Times (s): 0.0441\n",
      "Loss 0.2200 | Train Acc 0.9929\n",
      "Epoch  82 | Val acc: 0.7960  | Times (s): 0.0441\n",
      "Loss 0.2326 | Train Acc 0.9857\n",
      "Epoch  83 | Val acc: 0.7940  | Times (s): 0.0440\n",
      "Loss 0.2220 | Train Acc 0.9929\n",
      "Epoch  84 | Val acc: 0.7960  | Times (s): 0.0440\n",
      "Loss 0.2307 | Train Acc 1.0000\n",
      "Epoch  85 | Val acc: 0.7960  | Times (s): 0.0440\n",
      "Loss 0.2170 | Train Acc 0.9929\n",
      "Epoch  86 | Val acc: 0.8020  | Times (s): 0.0440\n",
      "Loss 0.2257 | Train Acc 0.9929\n",
      "Epoch  87 | Val acc: 0.7960  | Times (s): 0.0440\n",
      "Loss 0.2148 | Train Acc 1.0000\n",
      "Epoch  88 | Val acc: 0.7980  | Times (s): 0.0439\n",
      "Loss 0.2022 | Train Acc 1.0000\n",
      "Epoch  89 | Val acc: 0.8000  | Times (s): 0.0440\n",
      "Loss 0.2194 | Train Acc 1.0000\n",
      "Epoch  90 | Val acc: 0.8000  | Times (s): 0.0440\n",
      "Loss 0.2108 | Train Acc 1.0000\n",
      "Epoch  91 | Val acc: 0.7920  | Times (s): 0.0440\n",
      "Loss 0.2064 | Train Acc 1.0000\n",
      "Epoch  92 | Val acc: 0.7920  | Times (s): 0.0440\n",
      "Loss 0.2092 | Train Acc 0.9929\n",
      "Epoch  93 | Val acc: 0.7900  | Times (s): 0.0440\n",
      "Loss 0.2040 | Train Acc 0.9929\n",
      "Epoch  94 | Val acc: 0.7880  | Times (s): 0.0440\n",
      "Loss 0.2034 | Train Acc 0.9929\n",
      "Epoch  95 | Val acc: 0.8000  | Times (s): 0.0440\n",
      "Loss 0.1982 | Train Acc 0.9929\n",
      "Epoch  96 | Val acc: 0.8000  | Times (s): 0.0439\n",
      "Loss 0.1992 | Train Acc 1.0000\n",
      "Epoch  97 | Val acc: 0.8000  | Times (s): 0.0439\n",
      "Loss 0.2051 | Train Acc 1.0000\n",
      "Epoch  98 | Val acc: 0.7960  | Times (s): 0.0439\n",
      "Loss 0.1932 | Train Acc 1.0000\n",
      "Epoch  99 | Val acc: 0.7940  | Times (s): 0.0439\n",
      "Test acc 0.8140\n"
     ]
    }
   ],
   "source": [
    "devive = 'cpu'\n",
    "\n",
    "model = GraphSAGE_batch(g.ndata['feat'].shape[1], 256, dataset.num_classes).to(devive)\n",
    "train_batch(g, model, 100, lr=5e-3, weight_decay=5e-4, batch_size=1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "43d45a7d70284b3d42f2d3497b55b1f9f1462c7509e003c3de29480019cedae6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('dgl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
