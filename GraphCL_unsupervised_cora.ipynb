{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcpAE6yl8A3L",
        "outputId": "df5c78ca-6cd8-493f-b265-c518dbc84705"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.dgl.ai/wheels/repo.html\n",
            "Collecting dgl-cu111\n",
            "  Downloading https://data.dgl.ai/wheels/dgl_cu111-0.8.0.post1-cp37-cp37m-manylinux1_x86_64.whl (252.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 252.7 MB 56 kB/s \n",
            "\u001b[?25hCollecting dglgo\n",
            "  Downloading dglgo-0.0.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu111) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu111) (1.21.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from dgl-cu111) (4.63.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu111) (2.23.0)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl-cu111) (2.6.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu111) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu111) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu111) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu111) (2.10)\n",
            "Collecting isort>=5.10.1\n",
            "  Downloading isort-5.10.1-py3-none-any.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 15.0 MB/s \n",
            "\u001b[?25hCollecting typer>=0.4.0\n",
            "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
            "Collecting pydantic>=1.9.0\n",
            "  Downloading pydantic-1.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.9 MB 43.8 MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 50.3 MB/s \n",
            "\u001b[?25hCollecting numpydoc>=1.1.0\n",
            "  Downloading numpydoc-1.2-py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 7.3 MB/s \n",
            "\u001b[?25hCollecting autopep8>=1.6.0\n",
            "  Downloading autopep8-1.6.0-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.1 MB/s \n",
            "\u001b[?25hCollecting ruamel.yaml>=0.17.20\n",
            "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
            "\u001b[K     |████████████████████████████████| 109 kB 53.9 MB/s \n",
            "\u001b[?25hCollecting toml\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Collecting pycodestyle>=2.8.0\n",
            "  Downloading pycodestyle-2.8.0-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 875 kB/s \n",
            "\u001b[?25hRequirement already satisfied: sphinx>=1.8 in /usr/local/lib/python3.7/dist-packages (from numpydoc>=1.1.0->dglgo) (1.8.6)\n",
            "Requirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.7/dist-packages (from numpydoc>=1.1.0->dglgo) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10->numpydoc>=1.1.0->dglgo) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic>=1.9.0->dglgo) (3.10.0.2)\n",
            "Collecting ruamel.yaml.clib>=0.2.6\n",
            "  Downloading ruamel.yaml.clib-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (546 kB)\n",
            "\u001b[K     |████████████████████████████████| 546 kB 48.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=1.1.0->dglgo) (1.15.0)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=1.1.0->dglgo) (2.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=1.1.0->dglgo) (21.3)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=1.1.0->dglgo) (1.3.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=1.1.0->dglgo) (2.6.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=1.1.0->dglgo) (0.7.12)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=1.1.0->dglgo) (1.2.4)\n",
            "Requirement already satisfied: docutils<0.18,>=0.11 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=1.1.0->dglgo) (0.17.1)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=1.1.0->dglgo) (2.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=1.1.0->dglgo) (57.4.0)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel!=2.0,>=1.3->sphinx>=1.8->numpydoc>=1.1.0->dglgo) (2018.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer>=0.4.0->dglgo) (7.1.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->sphinx>=1.8->numpydoc>=1.1.0->dglgo) (3.0.7)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx>=1.8->numpydoc>=1.1.0->dglgo) (1.1.5)\n",
            "Installing collected packages: toml, ruamel.yaml.clib, pycodestyle, typer, ruamel.yaml, PyYAML, pydantic, numpydoc, isort, autopep8, dglgo, dgl-cu111\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-6.0 autopep8-1.6.0 dgl-cu111-0.8.0.post1 dglgo-0.0.1 isort-5.10.1 numpydoc-1.2 pycodestyle-2.8.0 pydantic-1.9.0 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.6 toml-0.10.2 typer-0.4.0\n",
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading /root/.dgl/cora_v2.zip from https://data.dgl.ai/dataset/cora_v2.zip...\n",
            "Extracting file to /root/.dgl/cora_v2\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done saving data into cached files.\n",
            "label shape: torch.Size([2708]) tensor([4, 4, 4,  ..., 4, 3, 3])\n"
          ]
        }
      ],
      "source": [
        "# 数据导入\n",
        "import torch\n",
        "!pip install dgl-cu111 dglgo -f https://data.dgl.ai/wheels/repo.html\n",
        "from dgl.data import CoraGraphDataset\n",
        "import scipy.sparse as sp\n",
        "\n",
        "\n",
        "dataset = CoraGraphDataset()\n",
        "graph = dataset[0]\n",
        "\n",
        "def norm_nodes(nodes):\n",
        "    \"节点特征归一化\"\n",
        "    feat = nodes.data['feat']\n",
        "    sum = feat.sum(1)\n",
        "    inv = 1. / sum\n",
        "    inv[torch.isinf(inv)] = 0.\n",
        "    norm = torch.mm(torch.diag(inv), feat)\n",
        "    return {'norm': norm}\n",
        "\n",
        "graph.apply_nodes(norm_nodes)\n",
        "\n",
        "label = graph.ndata['label']\n",
        "print('label shape:', label.shape, label) # 不是one-hot编码\n",
        "\n",
        "train_mask = graph.ndata['train_mask']\n",
        "val_mask = graph.ndata['val_mask']\n",
        "test_mask = graph.ndata['test_mask']\n",
        "\n",
        "train_idx = graph.nodes()[train_mask]\n",
        "val_idx = graph.nodes()[val_mask]\n",
        "test_idx = graph.nodes()[test_mask]\n",
        "\n",
        "graph.ndata['feat'] = graph.ndata.pop('norm')\n",
        "feat = graph.ndata['feat']\n",
        "adj = graph.adj(scipy_fmt='csr')\n",
        "\n",
        "num_nodes = graph.number_of_nodes()\n",
        "num_feats = feat.shape[1]\n",
        "num_classes = dataset.num_classes "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "S869D1KL8A3N"
      },
      "outputs": [],
      "source": [
        "hid_feats = 512\n",
        "lr = 1e-3\n",
        "lr2 = 1e-2\n",
        "l2_coef = .0\n",
        "aug_type = 'node' # node mask edge subgraph\n",
        "patience = 20\n",
        "epochs = 500\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda:0'\n",
        "else:\n",
        "    device = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0Fd1Yzv-4dS",
        "outputId": "40d224f4-6f8e-4935-884b-d0e1ca13486f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Mar 25 12:10:49 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P0   115W / 149W |   6270MiB / 11441MiB |     62%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "n4pA-OjT8A3N"
      },
      "outputs": [],
      "source": [
        "# 数据处理\n",
        "# 四种数据增强方法 data augumentation\n",
        "#   - Node dropping \n",
        "#   - Edge perturbation\n",
        "#   - Attribute masking\n",
        "#   - Subgraph: Random Walk\n",
        "\n",
        "import random\n",
        "import copy\n",
        "import dgl\n",
        "from dgl.transforms import DropNode, DropEdge, AddEdge\n",
        "from dgl.sampling import node2vec_random_walk\n",
        "\n",
        "def aug_random_mask(graph, drop_prob=0.2):\n",
        "    num_nodes = graph.number_of_nodes()\n",
        "    num_mask = int(num_nodes * drop_prob)\n",
        "    node_idx = list(range(num_nodes))\n",
        "    mask_idx = random.sample(node_idx, num_mask)\n",
        "    \n",
        "    zero = torch.zeros_like(graph.ndata['feat'][0])\n",
        "    aug_graph = copy.deepcopy(graph)\n",
        "    aug_graph.ndata['feat'][mask_idx] = zero # broadcast\n",
        "    \n",
        "    return aug_graph\n",
        "\n",
        "def aug_drop_node(graph, drop_prob=0.2):\n",
        "    transform = DropNode(p=drop_prob)\n",
        "    aug_graph = copy.deepcopy(graph)\n",
        "    aug_graph = transform(aug_graph)\n",
        "    \n",
        "    return aug_graph\n",
        "\n",
        "def aug_random_edge(graph, drop_prob=0.2):\n",
        "    num_edges = graph.number_of_edges()\n",
        "    num_drop_edges = int(num_edges * drop_prob)\n",
        "    add_ratio = num_drop_edges / (num_edges - num_drop_edges)\n",
        "    aug_graph = copy.deepcopy(graph)\n",
        "    \n",
        "    transform = DropEdge(p=drop_prob)\n",
        "    aug_graph = transform(aug_graph)\n",
        "    transform = AddEdge(ratio=add_ratio)\n",
        "    aug_graph = transform(aug_graph)\n",
        "    \n",
        "    return aug_graph\n",
        "\n",
        "def aug_subgraph(graph, drop_prob=0.2):\n",
        "    num_nodes = graph.number_of_nodes()\n",
        "    num_subgraph_nodes = int(num_nodes * (1 - drop_prob))\n",
        "    center_node_id = random.randint(0, num_nodes - 1)\n",
        "    \n",
        "    adj = graph.adj(scipy_fmt='csr')\n",
        "    \n",
        "    subgraph_idx = [center_node_id]\n",
        "    neighbor_idx = []\n",
        "    \n",
        "    for i in range(num_subgraph_nodes - 1):\n",
        "        neighbor_idx.extend(adj[subgraph_idx[i]].nonzero()[1]) # Random walk最后一个节点\n",
        "        neighbor_idx = list(set(neighbor_idx))\n",
        "        neighbor_idx = [idx for idx in neighbor_idx if idx not in subgraph_idx]\n",
        "    \n",
        "        if len(neighbor_idx) != 0:\n",
        "            new_node = random.sample(neighbor_idx, 1)[0]\n",
        "            subgraph_idx.append(new_node)\n",
        "        else:\n",
        "            break\n",
        "    \n",
        "    aug_graph = dgl.node_subgraph(graph, subgraph_idx)\n",
        "    return aug_graph\n",
        "\n",
        "\n",
        "\n",
        "if aug_type == 'node':\n",
        "    aug_view1 = aug_drop_node(graph)\n",
        "    aug_view2 = aug_drop_node(graph)\n",
        "elif aug_type == 'edge':\n",
        "    aug_view1 = aug_random_edge(graph)\n",
        "    aug_view2 = aug_random_edge(graph)\n",
        "elif aug_type == 'mask':\n",
        "    aug_view1 = aug_random_mask(graph)\n",
        "    aug_view2 = aug_random_mask(graph)\n",
        "elif aug_type == 'subgraph':\n",
        "    aug_view1 = aug_subgraph(graph)\n",
        "    aug_view2 = aug_subgraph(graph)\n",
        "else:\n",
        "    print('No aug type {}'.format(aug_type))\n",
        "    assert False\n",
        "    \n",
        "    \n",
        "graph = graph.add_self_loop()\n",
        "aug_view1 = aug_view1.add_self_loop()\n",
        "aug_view2 = aug_view2.add_self_loop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SlBRAu1o8A3P"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from dgl.nn import GraphConv, AvgPooling\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, hid_feats):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc = nn.Bilinear(hid_feats, hid_feats, 1)\n",
        "        \n",
        "    def forward(self, c, h_pl, h_mi):\n",
        "        # c 全局表示 h_pl 正例节点表示 h_mi 负例节点表示\n",
        "        # c_x = torch.unsqueeze(c, 1)\n",
        "        c_x = c.expand_as(h_pl).contiguous()\n",
        "        \n",
        "        sc_1 = torch.squeeze(self.fc(h_pl, c_x), 1)\n",
        "        sc_2 = torch.squeeze(self.fc(h_mi, c_x), 1)\n",
        "        \n",
        "        logits = torch.cat((sc_1, sc_2))\n",
        "        return logits\n",
        "\n",
        "class DGI(nn.Module):\n",
        "    def __init__(self, in_feats, hid_feats):\n",
        "        super(DGI, self).__init__()\n",
        "        self.gcn = GraphConv(in_feats, hid_feats, norm='both', bias=True, activation=nn.PReLU())\n",
        "        self.read = AvgPooling()\n",
        "        self.sigm = nn.Sigmoid()\n",
        "        self.disc = Discriminator(hid_feats)\n",
        "    \n",
        "    def forward(self, graph, aug_view1, aug_view2, shuf_feat):\n",
        "        h_0 = self.gcn(graph, graph.ndata['feat'])\n",
        "        h_2 = self.gcn(graph, shuf_feat)\n",
        "        \n",
        "        h_1 = self.gcn(aug_view1, aug_view1.ndata['feat'])\n",
        "        h_3 = self.gcn(aug_view2, aug_view2.ndata['feat'])\n",
        "        \n",
        "        c_1 = self.sigm(self.read(aug_view1, h_1))\n",
        "        c_3 = self.sigm(self.read(aug_view2, h_3))\n",
        "        \n",
        "        ret1 = self.disc(c_1, h_0, h_2)\n",
        "        ret2 = self.disc(c_3, h_0, h_2)\n",
        "        \n",
        "        return ret1 + ret2\n",
        "    \n",
        "    def get_embedding(self, graph):\n",
        "        h_1 = self.gcn(graph, graph.ndata['feat'])\n",
        "        c = self.read(graph, h_1)\n",
        "        \n",
        "        return h_1.detach(), c.detach()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXnL5zRK8A3P",
        "outputId": "38dd885d-673d-4b55-a16d-04e4b3617952"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 000, Loss: 0.6926\n",
            "Epoch: 001, Loss: 0.7176\n",
            "Epoch: 002, Loss: 0.6847\n",
            "Epoch: 003, Loss: 0.6917\n",
            "Epoch: 004, Loss: 0.6921\n",
            "Epoch: 005, Loss: 0.6857\n",
            "Epoch: 006, Loss: 0.6834\n",
            "Epoch: 007, Loss: 0.6854\n",
            "Epoch: 008, Loss: 0.6798\n",
            "Epoch: 009, Loss: 0.6744\n",
            "Epoch: 010, Loss: 0.6731\n",
            "Epoch: 011, Loss: 0.6687\n",
            "Epoch: 012, Loss: 0.6621\n",
            "Epoch: 013, Loss: 0.6565\n",
            "Epoch: 014, Loss: 0.6526\n",
            "Epoch: 015, Loss: 0.6431\n",
            "Epoch: 016, Loss: 0.6352\n",
            "Epoch: 017, Loss: 0.6259\n",
            "Epoch: 018, Loss: 0.6147\n",
            "Epoch: 019, Loss: 0.6080\n",
            "Epoch: 020, Loss: 0.5921\n",
            "Epoch: 021, Loss: 0.5823\n",
            "Epoch: 022, Loss: 0.5653\n",
            "Epoch: 023, Loss: 0.5606\n",
            "Epoch: 024, Loss: 0.5432\n",
            "Epoch: 025, Loss: 0.5243\n",
            "Epoch: 026, Loss: 0.5145\n",
            "Epoch: 027, Loss: 0.4969\n",
            "Epoch: 028, Loss: 0.4826\n",
            "Epoch: 029, Loss: 0.4662\n",
            "Epoch: 030, Loss: 0.4495\n",
            "Epoch: 031, Loss: 0.4419\n",
            "Epoch: 032, Loss: 0.4188\n",
            "Epoch: 033, Loss: 0.4053\n",
            "Epoch: 034, Loss: 0.3942\n",
            "Epoch: 035, Loss: 0.3974\n",
            "Epoch: 036, Loss: 0.3711\n",
            "Epoch: 037, Loss: 0.3572\n",
            "Epoch: 038, Loss: 0.3650\n",
            "Epoch: 039, Loss: 0.3401\n",
            "Epoch: 040, Loss: 0.3281\n",
            "Epoch: 041, Loss: 0.3173\n",
            "Epoch: 042, Loss: 0.3068\n",
            "Epoch: 043, Loss: 0.2990\n",
            "Epoch: 044, Loss: 0.2888\n",
            "Epoch: 045, Loss: 0.2772\n",
            "Epoch: 046, Loss: 0.2736\n",
            "Epoch: 047, Loss: 0.2665\n",
            "Epoch: 048, Loss: 0.2514\n",
            "Epoch: 049, Loss: 0.2479\n",
            "Epoch: 050, Loss: 0.2391\n",
            "Epoch: 051, Loss: 0.2351\n",
            "Epoch: 052, Loss: 0.2314\n",
            "Epoch: 053, Loss: 0.2209\n",
            "Epoch: 054, Loss: 0.2159\n",
            "Epoch: 055, Loss: 0.2132\n",
            "Epoch: 056, Loss: 0.2060\n",
            "Epoch: 057, Loss: 0.1981\n",
            "Epoch: 058, Loss: 0.1933\n",
            "Epoch: 059, Loss: 0.1977\n",
            "Epoch: 060, Loss: 0.1803\n",
            "Epoch: 061, Loss: 0.1896\n",
            "Epoch: 062, Loss: 0.1810\n",
            "Epoch: 063, Loss: 0.1819\n",
            "Epoch: 064, Loss: 0.1753\n",
            "Epoch: 065, Loss: 0.1623\n",
            "Epoch: 066, Loss: 0.1604\n",
            "Epoch: 067, Loss: 0.1620\n",
            "Epoch: 068, Loss: 0.1609\n",
            "Epoch: 069, Loss: 0.1494\n",
            "Epoch: 070, Loss: 0.1512\n",
            "Epoch: 071, Loss: 0.1546\n",
            "Epoch: 072, Loss: 0.1411\n",
            "Epoch: 073, Loss: 0.1308\n",
            "Epoch: 074, Loss: 0.1512\n",
            "Epoch: 075, Loss: 0.1379\n",
            "Epoch: 076, Loss: 0.1458\n",
            "Epoch: 077, Loss: 0.1383\n",
            "Epoch: 078, Loss: 0.1380\n",
            "Epoch: 079, Loss: 0.1428\n",
            "Epoch: 080, Loss: 0.1256\n",
            "Epoch: 081, Loss: 0.1416\n",
            "Epoch: 082, Loss: 0.1312\n",
            "Epoch: 083, Loss: 0.1258\n",
            "Epoch: 084, Loss: 0.1332\n",
            "Epoch: 085, Loss: 0.1096\n",
            "Epoch: 086, Loss: 0.1323\n",
            "Epoch: 087, Loss: 0.1118\n",
            "Epoch: 088, Loss: 0.1201\n",
            "Epoch: 089, Loss: 0.1078\n",
            "Epoch: 090, Loss: 0.1243\n",
            "Epoch: 091, Loss: 0.1054\n",
            "Epoch: 092, Loss: 0.0988\n",
            "Epoch: 093, Loss: 0.1064\n",
            "Epoch: 094, Loss: 0.1069\n",
            "Epoch: 095, Loss: 0.1093\n",
            "Epoch: 096, Loss: 0.1054\n",
            "Epoch: 097, Loss: 0.1019\n",
            "Epoch: 098, Loss: 0.1059\n",
            "Epoch: 099, Loss: 0.0989\n",
            "Epoch: 100, Loss: 0.0941\n",
            "Epoch: 101, Loss: 0.1025\n",
            "Epoch: 102, Loss: 0.0942\n",
            "Epoch: 103, Loss: 0.1009\n",
            "Epoch: 104, Loss: 0.0892\n",
            "Epoch: 105, Loss: 0.0991\n",
            "Epoch: 106, Loss: 0.0899\n",
            "Epoch: 107, Loss: 0.0912\n",
            "Epoch: 108, Loss: 0.0840\n",
            "Epoch: 109, Loss: 0.0978\n",
            "Epoch: 110, Loss: 0.0863\n",
            "Epoch: 111, Loss: 0.0903\n",
            "Epoch: 112, Loss: 0.0781\n",
            "Epoch: 113, Loss: 0.0986\n",
            "Epoch: 114, Loss: 0.0896\n",
            "Epoch: 115, Loss: 0.0897\n",
            "Epoch: 116, Loss: 0.0779\n",
            "Epoch: 117, Loss: 0.0881\n",
            "Epoch: 118, Loss: 0.0749\n",
            "Epoch: 119, Loss: 0.0856\n",
            "Epoch: 120, Loss: 0.0742\n",
            "Epoch: 121, Loss: 0.0842\n",
            "Epoch: 122, Loss: 0.0718\n",
            "Epoch: 123, Loss: 0.0785\n",
            "Epoch: 124, Loss: 0.0657\n",
            "Epoch: 125, Loss: 0.0769\n",
            "Epoch: 126, Loss: 0.0772\n",
            "Epoch: 127, Loss: 0.0884\n",
            "Epoch: 128, Loss: 0.0687\n",
            "Epoch: 129, Loss: 0.0825\n",
            "Epoch: 130, Loss: 0.0715\n",
            "Epoch: 131, Loss: 0.0797\n",
            "Epoch: 132, Loss: 0.0624\n",
            "Epoch: 133, Loss: 0.0769\n",
            "Epoch: 134, Loss: 0.0626\n",
            "Epoch: 135, Loss: 0.0735\n",
            "Epoch: 136, Loss: 0.0608\n",
            "Epoch: 137, Loss: 0.0756\n",
            "Epoch: 138, Loss: 0.0653\n",
            "Epoch: 139, Loss: 0.0655\n",
            "Epoch: 140, Loss: 0.0652\n",
            "Epoch: 141, Loss: 0.0649\n",
            "Epoch: 142, Loss: 0.0630\n",
            "Epoch: 143, Loss: 0.0607\n",
            "Epoch: 144, Loss: 0.0622\n",
            "Epoch: 145, Loss: 0.0584\n",
            "Epoch: 146, Loss: 0.0615\n",
            "Epoch: 147, Loss: 0.0594\n",
            "Epoch: 148, Loss: 0.0603\n",
            "Epoch: 149, Loss: 0.0551\n",
            "Epoch: 150, Loss: 0.0614\n",
            "Epoch: 151, Loss: 0.0592\n",
            "Epoch: 152, Loss: 0.0599\n",
            "Epoch: 153, Loss: 0.0508\n",
            "Epoch: 154, Loss: 0.0573\n",
            "Epoch: 155, Loss: 0.0536\n",
            "Epoch: 156, Loss: 0.0537\n",
            "Epoch: 157, Loss: 0.0500\n",
            "Epoch: 158, Loss: 0.0535\n",
            "Epoch: 159, Loss: 0.0469\n",
            "Epoch: 160, Loss: 0.0503\n",
            "Epoch: 161, Loss: 0.0460\n",
            "Epoch: 162, Loss: 0.0396\n",
            "Epoch: 163, Loss: 0.0491\n",
            "Epoch: 164, Loss: 0.0465\n",
            "Epoch: 165, Loss: 0.0480\n",
            "Epoch: 166, Loss: 0.0494\n",
            "Epoch: 167, Loss: 0.0535\n",
            "Epoch: 168, Loss: 0.0477\n",
            "Epoch: 169, Loss: 0.0468\n",
            "Epoch: 170, Loss: 0.0473\n",
            "Epoch: 171, Loss: 0.0434\n",
            "Epoch: 172, Loss: 0.0415\n",
            "Epoch: 173, Loss: 0.0438\n",
            "Epoch: 174, Loss: 0.0450\n",
            "Epoch: 175, Loss: 0.0458\n",
            "Epoch: 176, Loss: 0.0401\n",
            "Epoch: 177, Loss: 0.0432\n",
            "Epoch: 178, Loss: 0.0428\n",
            "Epoch: 179, Loss: 0.0418\n",
            "Epoch: 180, Loss: 0.0426\n",
            "Epoch: 181, Loss: 0.0422\n",
            "Epoch: 182, Loss: 0.0443\n",
            "Early stopping!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "model = DGI(num_feats, hid_feats)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_coef)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model.to(device)\n",
        "graph = graph.to(device)\n",
        "aug_view1 = aug_view1.to(device)\n",
        "aug_view2 = aug_view2.to(device)\n",
        "\n",
        "\n",
        "best = float('inf')\n",
        "cnt_wait = 0\n",
        "\n",
        "for e in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    shuf_idx = np.random.permutation(num_nodes)\n",
        "    shuf_feat = feat[shuf_idx, :]\n",
        "    \n",
        "    lbl1 = torch.ones(num_nodes)\n",
        "    lbl2 = torch.zeros(num_nodes)\n",
        "    lbl = torch.cat((lbl1, lbl2), 0)\n",
        "\n",
        "    lbl = lbl.to(device)\n",
        "    shuf_feat = shuf_feat.to(device)\n",
        "    \n",
        "    logits = model(graph, aug_view1, aug_view2, shuf_feat)\n",
        "    loss = loss_fn(logits, lbl)\n",
        "    print('Epoch: {:03d}, Loss: {:.4f}'.format(e, loss.item()))\n",
        "    \n",
        "    if loss < best:\n",
        "        best = loss\n",
        "        cnt_wait = 0\n",
        "        torch.save(model.state_dict(), 'graphcl.pkl')\n",
        "    else:\n",
        "        cnt_wait += 1\n",
        "        \n",
        "    if cnt_wait == patience:\n",
        "        print('Early stopping!')\n",
        "        break\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "model.load_state_dict(torch.load('graphcl.pkl'))\n",
        "embeds, _ = model.get_embedding(graph)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot2yR4fL8A3Q",
        "outputId": "c81c6d47-c385-45ba-d843-f06062026859"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "82.07000732421875 0.0823286846280098\n"
          ]
        }
      ],
      "source": [
        "# down stream task\n",
        "# node classification 只有节点的嵌入\n",
        "\n",
        "# model\n",
        "class LogReg(nn.Module):\n",
        "    def __init__(self, hid_dim, n_classes):\n",
        "        super(LogReg, self).__init__()\n",
        "        \n",
        "        self.fc = nn.Linear(hid_dim, n_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h = self.fc(x)\n",
        "        # h = torch.log_softmax(h, dim=-1)\n",
        "        return h\n",
        "\n",
        "# evaluation \n",
        "embeds = embeds.to('cpu')\n",
        "train_embs = embeds[train_idx]\n",
        "test_embs = embeds[test_idx]\n",
        "\n",
        "\n",
        "train_labels = label[train_idx]\n",
        "test_labels = label[test_idx]\n",
        "accs = []\n",
        "\n",
        "for _ in range(10):\n",
        "  model = LogReg(hid_feats, num_classes)\n",
        "\n",
        "  opt = torch.optim.Adam(model.parameters(), lr=lr2, weight_decay=l2_coef)\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        "\n",
        "  for epoch in range(300):\n",
        "    model.train()\n",
        "\n",
        "    logits = model(train_embs)\n",
        "    loss = loss_fn(logits, train_labels) # target可以接受 shape (N) 或 (N, d_1, ..., d_k)\n",
        "\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "  model.eval()\n",
        "  logits = model(test_embs)\n",
        "  preds = torch.argmax(logits, dim=1)\n",
        "  acc = torch.sum(preds == test_labels).float() / test_labels.shape[0]\n",
        "  accs.append(acc * 100)\n",
        "\n",
        "accs = torch.stack(accs) # Concatenates a sequence of tensors along a new dimension. 类型转换\n",
        "print(accs.mean().item(), accs.std().item())\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Du-TZE48_Khp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "43d45a7d70284b3d42f2d3497b55b1f9f1462c7509e003c3de29480019cedae6"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('dgl')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "GraphCL-dgl.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}