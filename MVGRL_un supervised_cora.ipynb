{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LG_PnYgh1ksZ",
        "outputId": "c7b47ddf-4d9d-4572-8f81-170ee68449ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.dgl.ai/wheels/repo.html\n",
            "Requirement already satisfied: dgl-cu111 in /usr/local/lib/python3.7/dist-packages (0.8.0.post1)\n",
            "Requirement already satisfied: dglgo in /usr/local/lib/python3.7/dist-packages (0.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from dgl-cu111) (4.63.0)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl-cu111) (2.6.3)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu111) (1.21.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu111) (2.23.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu111) (1.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu111) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu111) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu111) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu111) (1.24.3)\n",
            "Requirement already satisfied: typer>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from dglgo) (0.4.0)\n",
            "Requirement already satisfied: autopep8>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from dglgo) (1.6.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.7/dist-packages (from dglgo) (6.0)\n",
            "Requirement already satisfied: pydantic>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from dglgo) (1.9.0)\n",
            "Requirement already satisfied: isort>=5.10.1 in /usr/local/lib/python3.7/dist-packages (from dglgo) (5.10.1)\n",
            "Requirement already satisfied: ruamel.yaml>=0.17.20 in /usr/local/lib/python3.7/dist-packages (from dglgo) (0.17.21)\n",
            "Requirement already satisfied: numpydoc>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dglgo) (1.2)\n",
            "Requirement already satisfied: pycodestyle>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from autopep8>=1.6.0->dglgo) (2.8.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from autopep8>=1.6.0->dglgo) (0.10.2)\n",
            "Requirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.7/dist-packages (from numpydoc>=1.1.0->dglgo) (2.11.3)\n",
            "Requirement already satisfied: sphinx>=1.8 in /usr/local/lib/python3.7/dist-packages (from numpydoc>=1.1.0->dglgo) (1.8.6)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10->numpydoc>=1.1.0->dglgo) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic>=1.9.0->dglgo) (3.10.0.2)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.6 in /usr/local/lib/python3.7/dist-packages (from ruamel.yaml>=0.17.20->dglgo) (0.2.6)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=1.1.0->dglgo) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=1.1.0->dglgo) (57.4.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=1.1.0->dglgo) (0.7.12)\n",
            "Requirement already satisfied: docutils<0.18,>=0.11 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=1.1.0->dglgo) (0.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=1.1.0->dglgo) (1.15.0)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=1.1.0->dglgo) (1.2.4)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=1.1.0->dglgo) (2.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=1.1.0->dglgo) (21.3)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=1.1.0->dglgo) (2.9.1)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.8->numpydoc>=1.1.0->dglgo) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel!=2.0,>=1.3->sphinx>=1.8->numpydoc>=1.1.0->dglgo) (2018.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer>=0.4.0->dglgo) (7.1.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->sphinx>=1.8->numpydoc>=1.1.0->dglgo) (3.0.7)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx>=1.8->numpydoc>=1.1.0->dglgo) (1.1.5)\n"
          ]
        }
      ],
      "source": [
        "import torch as th\n",
        "import torch.nn as nn\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "# scipy 科学计算包\n",
        "import scipy.sparse as sp \n",
        "from scipy.linalg import fractional_matrix_power, inv\n",
        "# fractional_matrix_power: Compute the fractional power of a matrix.\n",
        "\n",
        "!pip install dgl-cu111 dglgo -f https://data.dgl.ai/wheels/repo.html\n",
        "\n",
        "import dgl \n",
        "from dgl.data import CoraGraphDataset, CiteseerGraphDataset, PubmedGraphDataset\n",
        "from dgl.nn import APPNPConv, GraphConv, AvgPooling\n",
        "# https://docs.dgl.ai/generated/dgl.nn.pytorch.conv.APPNPConv.html\n",
        "\n",
        "import networkx as nx\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -V\n",
        "!python -V"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvuGv9EI3FZk",
        "outputId": "7dff8500-4ad3-4e93-9954-3c3ec33a2a33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Mon_Oct_12_20:09:46_PDT_2020\n",
            "Cuda compilation tools, release 11.1, V11.1.105\n",
            "Build cuda_11.1.TC455_06.29190527_0\n",
            "Python 3.7.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72nFhOwI1ksc"
      },
      "source": [
        "$S^{PPR} = \\alpha{(} I_n - (1-\\alpha) {D^{-\\frac{1}{2}}} A {D^{-\\frac{1}{2}} )}^{-1}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mw-LwgUL1ksk"
      },
      "outputs": [],
      "source": [
        "def compute_ppr(graph: nx.Graph, alpha = 0.2, self_loop=True):\n",
        "    adj = nx.convert_matrix.to_numpy_array(graph) # Returns the graph adjacency matrix as a NumPy array.\n",
        "    if self_loop:\n",
        "        adj = adj + np.eye(adj.shape[0])\n",
        "            \n",
        "    deg = np.diag(np.sum(adj, 1))\n",
        "    deg_inv = fractional_matrix_power(deg, -0.5)\n",
        "    adj_norm = np.matmul(np.matmul(deg_inv, adj), deg_inv)\n",
        "    return alpha * inv(np.eye(adj.shape[0]) - (1 - alpha) * adj_norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpKyLrNG1ksl"
      },
      "outputs": [],
      "source": [
        "# dataset\n",
        "\n",
        "def process_dataset(name):\n",
        "    if name == 'cora':\n",
        "        dataset = CoraGraphDataset()\n",
        "    else:\n",
        "        # TODO: other datasets\n",
        "        pass    \n",
        "        \n",
        "    graph = dataset[0]    \n",
        "    feat = graph.ndata.pop('feat')\n",
        "    label = graph.ndata.pop('label')\n",
        "    \n",
        "    train_mask = graph.ndata.pop('train_mask')\n",
        "    val_mask = graph.ndata.pop('val_mask')\n",
        "    test_mask = graph.ndata.pop('test_mask')\n",
        "    \n",
        "    train_idx = th.nonzero(train_mask).squeeze()    \n",
        "    # 等效：train_nids = graph.nodes()[train_mask]\n",
        "    val_idx = th.nonzero(val_mask).squeeze()\n",
        "    test_idx = th.nonzero(test_mask).squeeze()\n",
        "    \n",
        "    # https://docs.dgl.ai/generated/dgl.add_self_loop.html\n",
        "    graph = graph.remove_self_loop()\n",
        "    graph = graph.add_self_loop()\n",
        "    \n",
        "    nx_g = dgl.to_networkx(graph)\n",
        "    \n",
        "    # diffusion matrices\n",
        "    diff_adj = compute_ppr(nx_g, 0.2, self_loop=False)\n",
        "    \n",
        "    diff_edges = np.nonzero(diff_adj) # ( array( ... ), array( ... ))\n",
        "    diff_weight = th.Tensor(diff_adj[diff_edges])\n",
        "    diff_graph = dgl.graph(diff_edges)\n",
        "\n",
        "    return graph, diff_graph, feat, label, train_idx, val_idx, test_idx, diff_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbnFQDBm1ksm"
      },
      "outputs": [],
      "source": [
        "# process_dataset('cora')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfNfUHoV1ksn"
      },
      "outputs": [],
      "source": [
        "# model\n",
        "    \n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fn = nn.Bilinear(dim, dim, 1)\n",
        "        \n",
        "    def forward(self, h1, h2, h3, h4, c1, c2):\n",
        "        \"\"\"\n",
        "        Mutual Information is modeled as discriminator \n",
        "        that takes in a node representation from one view and a graph representation from another view, \n",
        "        and scores the agreement between them.\n",
        "\n",
        "        Args:\n",
        "            h1 : view1 node representation (joint)\n",
        "            h2 : view2 node representation (joint)\n",
        "            h3 : view1 node representation (marginal)\n",
        "            h4 : view2 node representation (marginal)\n",
        "            c1 : view1 graph representation (joint)\n",
        "            c2 : view2 graph representation (joint)\n",
        "        \"\"\"\n",
        "        c_x1 = c1.expand_as(h1).contiguous()\n",
        "        c_x2 = c2.expand_as(h2).contiguous()\n",
        "        \n",
        "        # positive\n",
        "        sc_1 = self.fn(h2, c_x1).squeeze(1)\n",
        "        sc_2 = self.fn(h1, c_x2).squeeze(1)\n",
        "        \n",
        "         # negative\n",
        "        sc_3 = self.fn(h4, c_x1).squeeze(1)\n",
        "        sc_4 = self.fn(h3, c_x2).squeeze(1)\n",
        "\n",
        "        logits = th.cat((sc_1, sc_2, sc_3, sc_4))\n",
        "        return logits\n",
        "        \n",
        "class MVGRL(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super(MVGRL, self).__init__()\n",
        "        # https://docs.dgl.ai/generated/dgl.nn.pytorch.conv.GraphConv.html\n",
        "        # σ(A ̃XΘ) learn node representation for view1, A ̃ is symmetrically normalized adjacency matrix\n",
        "        self.encoder1 = GraphConv(in_dim, out_dim, norm='both', bias=True, activation=nn.PReLU())\n",
        "        # σ(SXΘ) learn node representation for view2, S is diffusion matrix\n",
        "        self.encoder2 = GraphConv(in_dim, out_dim, norm='none', bias=True, activation=nn.PReLU())\n",
        "        \n",
        "        # before the projection head, into a graph representation using a graph pooling (readout) function P\n",
        "        self.pooling = AvgPooling()\n",
        "        \n",
        "        self.disc = Discriminator(out_dim)\n",
        "        self.act_fn = nn.Sigmoid()\n",
        "        \n",
        "    def get_embedding(self, graph, diff_graph, feat, edge_weight):\n",
        "        h1 = self.encoder1(graph, feat)\n",
        "        h2 = self.encoder2(diff_graph, feat, edge_weight=edge_weight)\n",
        "        \n",
        "        return (h1 + h2).detach()\n",
        "    \n",
        "    def forward(self, graph, diff_graph, feat, shuf_feat, edge_weight):\n",
        "        h1 = self.encoder1(graph, feat)\n",
        "        h2 = self.encoder2(diff_graph, feat, edge_weight=edge_weight)\n",
        "        \n",
        "        h3 = self.encoder1(graph, shuf_feat)\n",
        "        h4 = self.encoder2(diff_graph, shuf_feat, edge_weight=edge_weight)\n",
        "        \n",
        "        c1 = self.act_fn(self.pooling(graph, h1))\n",
        "        c2 = self.act_fn(self.pooling(graph, h2))\n",
        "        \n",
        "        out = self.disc(h1, h2, h3, h4, c1, c2)\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlpH860s1ksq"
      },
      "outputs": [],
      "source": [
        "# 参数\n",
        "hid_dim = 512\n",
        "epochs = 50\n",
        "patience = 20 # Patient epochs to wait before early stopping.\n",
        "lr1 = 1e-3 # Learning rate of mvgrl\n",
        "lr2 = 1e-2 # Learning rate of linear evaluator\n",
        "wd1 = 5e-4\n",
        "wd2 = 5e-4\n",
        "device = 'cuda:0'\n",
        "sample_size = 2000"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GA75LHNI5M1z",
        "outputId": "2de9550d-2583-442a-862b-8f09c9481c0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Mar 19 07:44:29 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P8    35W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9BOyLlA1ksp",
        "outputId": "ae8e3fa3-9bf7-404d-dea1-8d5a8675f055"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  NumNodes: 2708\n",
            "  NumEdges: 10556\n",
            "  NumFeats: 1433\n",
            "  NumClasses: 7\n",
            "  NumTrainingSamples: 140\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "Done loading data from cached files.\n"
          ]
        }
      ],
      "source": [
        "# train\n",
        "\n",
        "# 1. prepare data \n",
        "graph, diff_graph, feat, label, train_idx, val_idx, test_idx, edge_weight = process_dataset('cora')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 子图采样\n",
        "graph.ndata['feat'] = feat\n",
        "diff_graph.edata['edge_weight'] = edge_weight\n",
        "\n",
        "n_feat = feat.shape[1]\n",
        "n_classes = np.unique(label).shape[0]\n",
        "\n",
        "train_idx = train_idx.to(device)\n",
        "val_idx = val_idx.to(device)\n",
        "test_idx = test_idx.to(device)\n",
        "\n",
        "n_node = graph.number_of_nodes()"
      ],
      "metadata": {
        "id": "uevF1Hfq6SNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. create model, optimizer, loss function\n",
        "model = MVGRL(n_feat, hid_dim)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = th.optim.Adam(model.parameters(), lr=lr1, weight_decay=wd1)\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss() \n",
        "# This loss combines a Sigmoid layer and the BCELoss in one single class.\n",
        "# https://cloud.tencent.com/developer/article/1660961"
      ],
      "metadata": {
        "id": "zL1n5Vx6BWCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "XttRaYkg1kss",
        "outputId": "35e80722-b13f-4dab-bf9e-9af94e41114d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 0.6937\n",
            "Epoch: 1, Loss: 0.6917\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-032582fd709d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    197\u001b[0m                                \"of them.\")\n\u001b[1;32m    198\u001b[0m         \u001b[0muser_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvjp_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbackward_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0muser_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_jvp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_bwd\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_bwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fwd_used_autocast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mbwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_bwd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/dgl/backend/pytorch/sparse.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, dZ)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreduce_op\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sum'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mop\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mul'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                     \u001b[0mdX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgspmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_rev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mul'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mop\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'add'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                     \u001b[0mdX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgspmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_rev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'copy_lhs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/dgl/backend/pytorch/sparse.py\u001b[0m in \u001b[0;36mgspmm\u001b[0;34m(gidx, op, reduce_op, lhs_data, rhs_data)\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mul'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mrhs_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrhs_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mGSpMM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlhs_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgsddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlhs_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlhs_target\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'u'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrhs_target\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'v'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_fwd\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_cast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcast_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_cast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcast_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_fwd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/dgl/backend/pytorch/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, gidx, op, reduce_op, X, Y)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcustom_fwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_gspmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgidx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0mreduce_last\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_need_reduce_last_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mX_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/dgl/sparse.py\u001b[0m in \u001b[0;36m_gspmm\u001b[0;34m(gidx, op, reduce_op, u, e)\u001b[0m\n\u001b[1;32m    231\u001b[0m                             \u001b[0mto_dgl_nd_for_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                             \u001b[0marg_u_nd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m                             arg_e_nd)\n\u001b[0m\u001b[1;32m    234\u001b[0m     \u001b[0;31m# NOTE(zihao): actually we can avoid the following step, because arg_*_nd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;31m# refers to the data that stores arg_*. After we call _CAPI_DGLKernelSpMM,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 4. training epochs\n",
        "\n",
        "lbl1 = th.ones(n_node * 2)\n",
        "lbl2 = th.zeros(n_node * 2)\n",
        "lbl = th.cat((lbl1, lbl2))\n",
        "lbl = lbl.to(device)\n",
        "\n",
        "# 把数据移动到device上\n",
        "graph = graph.to(device)\n",
        "diff_graph = diff_graph.to(device)\n",
        "feat = feat.to(device)\n",
        "edge_weight = edge_weight.to(device)\n",
        "\n",
        "best = float('inf')\n",
        "cnt_wait = 0 # early stopping\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    \n",
        "    # 打乱节点特征，相当于从边缘分布中采样\n",
        "    shuf_idx = np.random.permutation(n_node)\n",
        "    shuf_feat = feat[shuf_idx, :]\n",
        "    shuf_feat = shuf_feat.to(device)\n",
        "\n",
        "    logits = model(graph, diff_graph, feat, shuf_feat, edge_weight)\n",
        "    loss = loss_fn(logits, lbl)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    print('Epoch: {0}, Loss: {1:0.4f}'.format(epoch, loss.item()))\n",
        "    \n",
        "    if loss < best:\n",
        "        best = loss\n",
        "        cnt_wait = 0\n",
        "        th.save(model.state_dict(), 'mvgrl.pkl') # 保存模型参数，不保存模型结构\n",
        "    else: \n",
        "        cnt_wait += 1\n",
        "        \n",
        "    if cnt_wait == patience:\n",
        "        print('Early stopping')\n",
        "        break\n",
        "    \n",
        "model.load_state_dict(th.load('mvgrl.pkl'))\n",
        "embeds = model.get_embedding(graph, diff_graph, feat, edge_weight)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample 采样子图\n",
        "\n",
        "lbl1 = th.ones(sample_size * 2)\n",
        "lbl2 = th.zeros(sample_size * 2)\n",
        "lbl = th.cat((lbl1, lbl2))\n",
        "lbl = lbl.to(device)\n",
        "\n",
        "node_list = list(range(n_node))\n",
        "\n",
        "best = float('inf')\n",
        "cnt_wait = 0 # early stopping\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "\n",
        "    sample_idx = random.sample(node_list, sample_size)\n",
        "\n",
        "    sub_graph = dgl.node_subgraph(graph, sample_idx)\n",
        "    sub_diff_graph = dgl.node_subgraph(diff_graph, sample_idx)\n",
        "\n",
        "    f = sub_graph.ndata.pop('feat')\n",
        "    ew = sub_diff_graph.edata.pop('edge_weight')\n",
        "\n",
        "    shuf_idx = np.random.permutation(sample_size)\n",
        "    sf = feat[shuf_idx, :]\n",
        "\n",
        "    sub_graph = sub_graph.to(device)\n",
        "    sub_diff_graph = sub_diff_graph.to(device)\n",
        "    f = f.to(device)\n",
        "    ew = ew.to(device)\n",
        "    sf = sf.to(device)\n",
        "\n",
        "    logits = model(sub_graph, sub_diff_graph, f, sf, ew)\n",
        "    loss = loss_fn(logits, lbl)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print('Epoch: {0}, Loss: {1:0.4f}'.format(epoch, loss.item()))\n",
        "    \n",
        "    if loss < best:\n",
        "        best = loss\n",
        "        cnt_wait = 0\n",
        "        th.save(model.state_dict(), 'mvgrl.pkl') # 保存模型参数，不保存模型结构\n",
        "    else: \n",
        "        cnt_wait += 1\n",
        "        \n",
        "    if cnt_wait == patience:\n",
        "        print('Early stopping')\n",
        "        break\n",
        "\n",
        "model.load_state_dict(th.load('mvgrl.pkl'))\n",
        "\n",
        "# 把数据移动到device上\n",
        "graph = graph.to(device)\n",
        "diff_graph = diff_graph.to(device)\n",
        "feat = feat.to(device)\n",
        "edge_weight = edge_weight.to(device)\n",
        "embeds = model.get_embedding(graph, diff_graph, feat, edge_weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CikRdTqBEeAs",
        "outputId": "db4cc3d0-c2c2-4934-c146-3f73f641ccd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 0.3746\n",
            "Epoch: 1, Loss: 0.3621\n",
            "Epoch: 2, Loss: 0.3546\n",
            "Epoch: 3, Loss: 0.3579\n",
            "Epoch: 4, Loss: 0.3452\n",
            "Epoch: 5, Loss: 0.3453\n",
            "Epoch: 6, Loss: 0.3404\n",
            "Epoch: 7, Loss: 0.3267\n",
            "Epoch: 8, Loss: 0.3364\n",
            "Epoch: 9, Loss: 0.3303\n",
            "Epoch: 10, Loss: 0.3197\n",
            "Epoch: 11, Loss: 0.3282\n",
            "Epoch: 12, Loss: 0.3228\n",
            "Epoch: 13, Loss: 0.3088\n",
            "Epoch: 14, Loss: 0.3155\n",
            "Epoch: 15, Loss: 0.3010\n",
            "Epoch: 16, Loss: 0.3232\n",
            "Epoch: 17, Loss: 0.3130\n",
            "Epoch: 18, Loss: 0.3072\n",
            "Epoch: 19, Loss: 0.3006\n",
            "Epoch: 20, Loss: 0.2960\n",
            "Epoch: 21, Loss: 0.2883\n",
            "Epoch: 22, Loss: 0.3067\n",
            "Epoch: 23, Loss: 0.2866\n",
            "Epoch: 24, Loss: 0.2895\n",
            "Epoch: 25, Loss: 0.2943\n",
            "Epoch: 26, Loss: 0.2912\n",
            "Epoch: 27, Loss: 0.2943\n",
            "Epoch: 28, Loss: 0.2913\n",
            "Epoch: 29, Loss: 0.2853\n",
            "Epoch: 30, Loss: 0.2751\n",
            "Epoch: 31, Loss: 0.2794\n",
            "Epoch: 32, Loss: 0.2922\n",
            "Epoch: 33, Loss: 0.2769\n",
            "Epoch: 34, Loss: 0.2701\n",
            "Epoch: 35, Loss: 0.2800\n",
            "Epoch: 36, Loss: 0.2867\n",
            "Epoch: 37, Loss: 0.2643\n",
            "Epoch: 38, Loss: 0.2783\n",
            "Epoch: 39, Loss: 0.2694\n",
            "Epoch: 40, Loss: 0.2741\n",
            "Epoch: 41, Loss: 0.2684\n",
            "Epoch: 42, Loss: 0.2732\n",
            "Epoch: 43, Loss: 0.2752\n",
            "Epoch: 44, Loss: 0.2592\n",
            "Epoch: 45, Loss: 0.2595\n",
            "Epoch: 46, Loss: 0.2744\n",
            "Epoch: 47, Loss: 0.2573\n",
            "Epoch: 48, Loss: 0.2730\n",
            "Epoch: 49, Loss: 0.2575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# down stream task\n",
        "# node classification 只有节点的嵌入\n",
        "\n",
        "# model\n",
        "class LogReg(nn.Module):\n",
        "    def __init__(self, hid_dim, n_classes):\n",
        "        super(LogReg, self).__init__()\n",
        "        \n",
        "        self.fc = nn.Linear(hid_dim, n_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h = self.fc(x)\n",
        "        # h = th.log_softmax(h, dim=-1)\n",
        "        return h\n",
        "\n",
        "# evaluation \n",
        "train_embs = embeds[train_idx]\n",
        "test_embs = embeds[test_idx]\n",
        "\n",
        "label = label.to(device)\n",
        "train_labels = label[train_idx]\n",
        "test_labels = label[test_idx]\n",
        "accs = []\n",
        "\n",
        "for _ in range(5):\n",
        "  model = LogReg(hid_dim, n_classes)\n",
        "  model = model.to(device)\n",
        "\n",
        "  opt = th.optim.Adam(model.parameters(), lr=lr2, weight_decay=wd2)\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        "\n",
        "  for epoch in range(300):\n",
        "    model.train()\n",
        "\n",
        "    logits = model(train_embs)\n",
        "    loss = loss_fn(logits, train_labels) # target可以接受 shape (N) 或 (N, d_1, ..., d_k)\n",
        "\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "  model.eval()\n",
        "  logits = model(test_embs)\n",
        "  preds = th.argmax(logits, dim=1)\n",
        "  acc = th.sum(preds == test_labels).float() / test_labels.shape[0]\n",
        "  accs.append(acc * 100)\n",
        "\n",
        "accs = th.stack(accs) # Concatenates a sequence of tensors along a new dimension. 类型转换\n",
        "print(accs.mean().item(), accs.std().item())\n",
        "  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noYzwjDC3lLi",
        "outputId": "a87fc94c-081f-4076-9648-3573d895bb51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79.80000305175781 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gXjcTlUX-zog"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "43d45a7d70284b3d42f2d3497b55b1f9f1462c7509e003c3de29480019cedae6"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('dgl')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "mvgrl.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}